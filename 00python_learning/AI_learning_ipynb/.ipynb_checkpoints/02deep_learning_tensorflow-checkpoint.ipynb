{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 12:02:44.888369  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(1, 2), dtype=float32) Tensor(\"Const_4:0\", shape=(2, 1), dtype=float32)\n",
      "Tensor(\"MatMul_1:0\", shape=(1, 1), dtype=float32)\n",
      "[array([[12.]], dtype=float32)]\n",
      "[-2. -1.]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "matrix1 = tf.constant([[3., 3.]])  \n",
    "matrix2 = tf.constant([[2.], [2.]])\n",
    "print(matrix1, matrix2)\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "print(product)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([product])\n",
    "    print(result)\n",
    "    \n",
    "# 交互式使用,使用 InteractiveSession 代替 Session 类， 使用Tensor.eval() 和 Operation.run() 方法代替 Session.run()\n",
    "sess = tf.InteractiveSession()\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "# 使用初始化器 initializer op 的 run() 方法初始化 'x' \n",
    "x.initializer.run()\n",
    "# 增加一个减法 subtract op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 \n",
    "sub = tf.subtract (x, a)\n",
    "print(sub.eval())\n",
    "\n",
    "# 变量\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value) # 将new_value赋值给state\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess: # 启动图, 运行 op\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(state))\n",
    "    for i in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1:\t b'helloworld!'\n",
      "test2:\t 7\n",
      "test3:\t [7, 12]\n",
      "test4:\t [7, 12]\n",
      "test5:\t [21.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('helloworld!')\n",
    "\n",
    "a = tf.placeholder(tf.int32)\n",
    "b = tf.placeholder(tf.int32)\n",
    "add = tf.add(a, b)\n",
    "\n",
    "mul = tf.multiply(a, b)\n",
    "with tf.Session() as sess:  # 在启动会话之前所有节点没有真正运行\n",
    "    print('test1:\\t', sess.run(hello))\n",
    "    print('test2:\\t', sess.run(add, feed_dict = {a:3, b:4}))  # feed机制将具体数值通过占位符传入，并进行运算\n",
    "    print('test3:\\t', sess.run([add, mul], feed_dict = {a:3, b:4}))\n",
    "    with tf.device(\"/gpu:1\"):  # 指定GPU1参与运算,“/cpu:0”: 机器的 CPU；0开始计数\n",
    "        print('test4:\\t', sess.run([add, mul], feed_dict={a:3, b:4}))\n",
    "        \n",
    "input1 = tf.constant(3.0)\n",
    "input2 = tf.constant(2.0)\n",
    "input3 = tf.constant(5.0)\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = tf.multiply (input1, intermed)\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print('test5:\\t', result)\n",
    "\n",
    "#  feed 机制，该机制可以临时替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁，直接插入一个 tensor。\n",
    "input1 = tf.placeholder(tf.types.float32)\n",
    "input2 = tf.placeholder(tf.types.float32)\n",
    "output = tf. multiply(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "  print('test6:\\t', sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 17:11:07.646332 11692 deprecation.py:323] From <ipython-input-2-15aaf13e95b1>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0801 17:11:07.647361 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0801 17:11:07.648813 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 17:11:07.992046 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0801 17:11:07.996510 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0801 17:11:08.068430 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "TFRecord格式训练文件已保存\n"
     ]
    }
   ],
   "source": [
    "#  TFRecord文件存储\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", dtype=tf.uint8, one_hot=True)\n",
    "images = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "\n",
    "def critical(value):\n",
    "    for i in range(5):\n",
    "        if value[i] == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "need_index = []\n",
    "for i,val in enumerate(labels):\n",
    "    if critical(val):\n",
    "        need_index.append(i)\n",
    "pixels = images.shape[1]\n",
    "num_examples = int(mnist.train.num_examples)\n",
    "\n",
    "filename = \"train.tfrecords\"\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for index in need_index:\n",
    "    image_raw = images[index].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        # 'pixels':_int64_featue(pixels),\n",
    "        'label':_int64_feature(np.argmax(labels[index])),\n",
    "        'img_raw':_bytes_feature(image_raw)}))  # example对象对label和image数据进行封装\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()\n",
    "print(\"TFRecord格式训练文件已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 17:11:11.660965 11692 deprecation.py:323] From <ipython-input-3-7995cb4dbcc4>:4: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0801 17:11:11.669886 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0801 17:11:11.673361 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W0801 17:11:11.681293 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0801 17:11:11.684269 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0801 17:11:11.693196 11692 deprecation.py:323] From <ipython-input-3-7995cb4dbcc4>:5: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "W0801 17:11:11.707097 11692 deprecation.py:323] From <ipython-input-3-7995cb4dbcc4>:27: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "W0801 17:11:11.935740 11692 deprecation.py:323] From <ipython-input-3-7995cb4dbcc4>:69: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000 training step(s), loss is 0.1805371791124344, and accuracy is 1.0\n",
      "After 4000 training step(s), loss is 0.2496822476387024, and accuracy is 0.800000011920929\n",
      "After 6000 training step(s), loss is 0.1695115864276886, and accuracy is 1.0\n",
      "After 8000 training step(s), loss is 0.15695570409297943, and accuracy is 1.0\n",
      "After 10000 training step(s), loss is 0.2267659455537796, and accuracy is 1.0\n",
      "After 12000 training step(s), loss is 0.18009954690933228, and accuracy is 1.0\n",
      "After 14000 training step(s), loss is 0.15718236565589905, and accuracy is 1.0\n",
      "After 16000 training step(s), loss is 0.15496237576007843, and accuracy is 1.0\n",
      "After 18000 training step(s), loss is 0.16410091519355774, and accuracy is 1.0\n",
      "After 20000 training step(s), loss is 0.764926552772522, and accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 解析一个TFRecord样本\n",
    "def parser(record):\n",
    "    queue = tf.train.string_input_producer([record])  # 根据TFRecord文件生成队列\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(queue)\n",
    "    features = tf.parse_single_example(serialized_example, features={\n",
    "        'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "    # tf.decode_raw() 用于将numpy array 解析成图像对应的像素数组\n",
    "    decoded_image = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    image = tf.reshape(decoded_image, [784])\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5  # 将像素值归一化到[-0.5, 0.5]\n",
    "    label = tf.cast(features['label'], tf.int64)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# 定义单隐层神经网络的前向传播过程，并返回神经网络的前向传播结果\n",
    "def inference(input_tensor, weights1, biases1, weights2, biases2):\n",
    "    layer = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "    return tf.matmul(layer, weights2) + biases2\n",
    "\n",
    "\n",
    "image, label = parser(\"train.tfrecords\")\n",
    "# 将处理后的图像和标签数据通过tf.train.shuffle_batch整理成ANN训练时所需要的batch\n",
    "img_batch, label_batch = tf.train.shuffle_batch([image, label], batch_size=5, capacity=1015, min_after_dequeue=500)\n",
    "\n",
    "# 神经网络模型的相关参数设置\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 5\n",
    "LAYER1_NODE = 500                        # 隐层神经元节点数\n",
    "LEARNING_RATE = 0.002                    # 初始学习率设定\n",
    "REGULARAZTION_RATE = 0.0001             # 正则化系数设定\n",
    "TRAINING_STEPS = 20000                    # 总训练步数\n",
    "current_path = r'C:/Users/Lenovo/Desktop/data/'\n",
    "MODEL_PATH = \"model/\"               # 定义模型保存地址与文件名\n",
    "MODEL_NAME = \"EXAMPLE.ckpt\"\n",
    "\n",
    "weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "y = inference(img_batch, weights1, biases1, weights2, biases2)\n",
    "\n",
    "# 计算交叉熵及其在当前训练batch上的平均值\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=label_batch)\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# 损失函数的计算，包含两项，其一：平均交叉熵损失，其二：正则化项\n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "loss = cross_entropy_mean + regularaztion\n",
    "\n",
    "# 优化损失函数，这里使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "# 准确率计算，先将精度转换为float32，再计算准确率\n",
    "prediction = tf.equal(tf.argmax(y, 1), label_batch)  # label_batch类似[1, 4, 1, 2, 0]\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "# 初始化TensorFlow持久化类，用于模型的保存\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  # 创建会话\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    coord = tf.train.Coordinator()  # 用于完成多线程协同的功能，该类实例需要明确调用下述语句来启动所有线程\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for i in range(1, TRAINING_STEPS + 1):\n",
    "        sess.run(train_step)\n",
    "        if i % 2000 == 0:\n",
    "            print(\"After {} training step(s), loss is {}, and accuracy is {}\".format(i, sess.run(loss), sess.run(accuracy)))\n",
    "            saver.save(sess, os.path.join(MODEL_PATH, MODEL_NAME))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)   # 用于停止所有进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 17:12:07.855276 11692 deprecation.py:506] From <ipython-input-4-d213aff81766>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0801 17:12:08.442575 11692 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 1000, training accuracy 0.98\n",
      "step 2000, training accuracy 0.94\n",
      "step 3000, training accuracy 0.9\n",
      "step 4000, training accuracy 0.98\n",
      "test accuracy 0.9778\n",
      "Const\n",
      "Placeholder\n",
      "Placeholder_1\n",
      "Add\n",
      "Mul\n",
      "input_producer/Const\n",
      "input_producer/Size\n",
      "input_producer/Greater/y\n",
      "input_producer/Greater\n",
      "input_producer/Assert/Const\n",
      "input_producer/Assert/Assert/data_0\n",
      "input_producer/Assert/Assert\n",
      "input_producer/Identity\n",
      "input_producer/RandomShuffle\n",
      "input_producer\n",
      "input_producer/input_producer_EnqueueMany\n",
      "input_producer/input_producer_Close\n",
      "input_producer/input_producer_Close_1\n",
      "input_producer/input_producer_Size\n",
      "input_producer/Cast\n",
      "input_producer/mul/y\n",
      "input_producer/mul\n",
      "input_producer/fraction_of_32_full/tags\n",
      "input_producer/fraction_of_32_full\n",
      "TFRecordReaderV2\n",
      "ReaderReadV2\n",
      "ParseSingleExample/Const\n",
      "ParseSingleExample/Const_1\n",
      "ParseSingleExample/ParseSingleExample\n",
      "DecodeRaw\n",
      "Reshape/shape\n",
      "Reshape\n",
      "Cast\n",
      "mul_1/y\n",
      "mul_1\n",
      "sub/y\n",
      "sub\n",
      "shuffle_batch/Const\n",
      "shuffle_batch/random_shuffle_queue\n",
      "shuffle_batch/random_shuffle_queue_enqueue\n",
      "shuffle_batch/random_shuffle_queue_Close\n",
      "shuffle_batch/random_shuffle_queue_Close_1\n",
      "shuffle_batch/random_shuffle_queue_Size\n",
      "shuffle_batch/sub/y\n",
      "shuffle_batch/sub\n",
      "shuffle_batch/Maximum/x\n",
      "shuffle_batch/Maximum\n",
      "shuffle_batch/Cast\n",
      "shuffle_batch/mul/y\n",
      "shuffle_batch/mul\n",
      "shuffle_batch/fraction_over_500_of_515_full/tags\n",
      "shuffle_batch/fraction_over_500_of_515_full\n",
      "shuffle_batch/n\n",
      "shuffle_batch\n",
      "truncated_normal/shape\n",
      "truncated_normal/mean\n",
      "truncated_normal/stddev\n",
      "truncated_normal/TruncatedNormal\n",
      "truncated_normal/mul\n",
      "truncated_normal\n",
      "Variable\n",
      "Variable/Assign\n",
      "Variable/read\n",
      "Const_1\n",
      "Variable_1\n",
      "Variable_1/Assign\n",
      "Variable_1/read\n",
      "truncated_normal_1/shape\n",
      "truncated_normal_1/mean\n",
      "truncated_normal_1/stddev\n",
      "truncated_normal_1/TruncatedNormal\n",
      "truncated_normal_1/mul\n",
      "truncated_normal_1\n",
      "Variable_2\n",
      "Variable_2/Assign\n",
      "Variable_2/read\n",
      "Const_2\n",
      "Variable_3\n",
      "Variable_3/Assign\n",
      "Variable_3/read\n",
      "MatMul\n",
      "add_1\n",
      "Relu\n",
      "MatMul_1\n",
      "add_2\n",
      "SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "Const_3\n",
      "Mean\n",
      "l2_regularizer/scale\n",
      "l2_regularizer/L2Loss\n",
      "l2_regularizer\n",
      "l2_regularizer_1/scale\n",
      "l2_regularizer_1/L2Loss\n",
      "l2_regularizer_1\n",
      "add_3\n",
      "add_4\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/add_4_grad/tuple/group_deps\n",
      "gradients/add_4_grad/tuple/control_dependency\n",
      "gradients/add_4_grad/tuple/control_dependency_1\n",
      "gradients/Mean_grad/Reshape/shape\n",
      "gradients/Mean_grad/Reshape\n",
      "gradients/Mean_grad/Const\n",
      "gradients/Mean_grad/Tile\n",
      "gradients/Mean_grad/Const_1\n",
      "gradients/Mean_grad/truediv\n",
      "gradients/add_3_grad/tuple/group_deps\n",
      "gradients/add_3_grad/tuple/control_dependency\n",
      "gradients/add_3_grad/tuple/control_dependency_1\n",
      "gradients/zeros_like\n",
      "gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/l2_regularizer_grad/Mul\n",
      "gradients/l2_regularizer_grad/Mul_1\n",
      "gradients/l2_regularizer_grad/tuple/group_deps\n",
      "gradients/l2_regularizer_grad/tuple/control_dependency\n",
      "gradients/l2_regularizer_grad/tuple/control_dependency_1\n",
      "gradients/l2_regularizer_1_grad/Mul\n",
      "gradients/l2_regularizer_1_grad/Mul_1\n",
      "gradients/l2_regularizer_1_grad/tuple/group_deps\n",
      "gradients/l2_regularizer_1_grad/tuple/control_dependency\n",
      "gradients/l2_regularizer_1_grad/tuple/control_dependency_1\n",
      "gradients/add_2_grad/Shape\n",
      "gradients/add_2_grad/Shape_1\n",
      "gradients/add_2_grad/BroadcastGradientArgs\n",
      "gradients/add_2_grad/Sum\n",
      "gradients/add_2_grad/Reshape\n",
      "gradients/add_2_grad/Sum_1\n",
      "gradients/add_2_grad/Reshape_1\n",
      "gradients/add_2_grad/tuple/group_deps\n",
      "gradients/add_2_grad/tuple/control_dependency\n",
      "gradients/add_2_grad/tuple/control_dependency_1\n",
      "gradients/l2_regularizer/L2Loss_grad/mul\n",
      "gradients/l2_regularizer_1/L2Loss_grad/mul\n",
      "gradients/MatMul_1_grad/MatMul\n",
      "gradients/MatMul_1_grad/MatMul_1\n",
      "gradients/MatMul_1_grad/tuple/group_deps\n",
      "gradients/MatMul_1_grad/tuple/control_dependency\n",
      "gradients/MatMul_1_grad/tuple/control_dependency_1\n",
      "gradients/Relu_grad/ReluGrad\n",
      "gradients/AddN\n",
      "gradients/add_1_grad/Shape\n",
      "gradients/add_1_grad/Shape_1\n",
      "gradients/add_1_grad/BroadcastGradientArgs\n",
      "gradients/add_1_grad/Sum\n",
      "gradients/add_1_grad/Reshape\n",
      "gradients/add_1_grad/Sum_1\n",
      "gradients/add_1_grad/Reshape_1\n",
      "gradients/add_1_grad/tuple/group_deps\n",
      "gradients/add_1_grad/tuple/control_dependency\n",
      "gradients/add_1_grad/tuple/control_dependency_1\n",
      "gradients/MatMul_grad/MatMul\n",
      "gradients/MatMul_grad/MatMul_1\n",
      "gradients/MatMul_grad/tuple/group_deps\n",
      "gradients/MatMul_grad/tuple/control_dependency\n",
      "gradients/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_1\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_Variable/ApplyGradientDescent\n",
      "GradientDescent/update_Variable_1/ApplyGradientDescent\n",
      "GradientDescent/update_Variable_2/ApplyGradientDescent\n",
      "GradientDescent/update_Variable_3/ApplyGradientDescent\n",
      "GradientDescent\n",
      "ArgMax/dimension\n",
      "ArgMax\n",
      "Equal\n",
      "Cast_1\n",
      "Const_4\n",
      "Mean_1\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/restore_all\n",
      "init\n",
      "init_1\n",
      "Placeholder_2/X_placeholder\n",
      "Placeholder_2/y_placeholder\n",
      "Placeholder_2/Reshape/shape\n",
      "Placeholder_2/Reshape\n",
      "conv1/W_conv1/Initializer/random_normal/shape\n",
      "conv1/W_conv1/Initializer/random_normal/mean\n",
      "conv1/W_conv1/Initializer/random_normal/stddev\n",
      "conv1/W_conv1/Initializer/random_normal/RandomStandardNormal\n",
      "conv1/W_conv1/Initializer/random_normal/mul\n",
      "conv1/W_conv1/Initializer/random_normal\n",
      "conv1/W_conv1\n",
      "conv1/W_conv1/Assign\n",
      "conv1/W_conv1/read\n",
      "conv1/b_conv1/Initializer/Const\n",
      "conv1/b_conv1\n",
      "conv1/b_conv1/Assign\n",
      "conv1/b_conv1/read\n",
      "conv1/Conv2D\n",
      "conv1/BiasAdd\n",
      "conv1/conv1\n",
      "pool1/pool1\n",
      "conv2/W_conv2/Initializer/random_normal/shape\n",
      "conv2/W_conv2/Initializer/random_normal/mean\n",
      "conv2/W_conv2/Initializer/random_normal/stddev\n",
      "conv2/W_conv2/Initializer/random_normal/RandomStandardNormal\n",
      "conv2/W_conv2/Initializer/random_normal/mul\n",
      "conv2/W_conv2/Initializer/random_normal\n",
      "conv2/W_conv2\n",
      "conv2/W_conv2/Assign\n",
      "conv2/W_conv2/read\n",
      "conv2/b_conv2/Initializer/Const\n",
      "conv2/b_conv2\n",
      "conv2/b_conv2/Assign\n",
      "conv2/b_conv2/read\n",
      "conv2/Conv2D\n",
      "conv2/BiasAdd\n",
      "conv2/conv2\n",
      "pool2/pool2\n",
      "fc1/W_fc1/Initializer/truncated_normal/shape\n",
      "fc1/W_fc1/Initializer/truncated_normal/mean\n",
      "fc1/W_fc1/Initializer/truncated_normal/stddev\n",
      "fc1/W_fc1/Initializer/truncated_normal/TruncatedNormal\n",
      "fc1/W_fc1/Initializer/truncated_normal/mul\n",
      "fc1/W_fc1/Initializer/truncated_normal\n",
      "fc1/W_fc1\n",
      "fc1/W_fc1/Assign\n",
      "fc1/W_fc1/read\n",
      "fc1/b_fc1/Initializer/Const\n",
      "fc1/b_fc1\n",
      "fc1/b_fc1/Assign\n",
      "fc1/b_fc1/read\n",
      "fc1/Reshape/shape\n",
      "fc1/Reshape\n",
      "fc1/MatMul\n",
      "fc1/add\n",
      "fc1/fc1\n",
      "fc1/dropout/rate\n",
      "fc1/dropout/Shape\n",
      "fc1/dropout/random_uniform/min\n",
      "fc1/dropout/random_uniform/max\n",
      "fc1/dropout/random_uniform/RandomUniform\n",
      "fc1/dropout/random_uniform/sub\n",
      "fc1/dropout/random_uniform/mul\n",
      "fc1/dropout/random_uniform\n",
      "fc1/dropout/sub/x\n",
      "fc1/dropout/sub\n",
      "fc1/dropout/truediv/x\n",
      "fc1/dropout/truediv\n",
      "fc1/dropout/GreaterEqual\n",
      "fc1/dropout/mul\n",
      "fc1/dropout/Cast\n",
      "fc1/dropout/mul_1\n",
      "softmax/W_softmax/Initializer/truncated_normal/shape\n",
      "softmax/W_softmax/Initializer/truncated_normal/mean\n",
      "softmax/W_softmax/Initializer/truncated_normal/stddev\n",
      "softmax/W_softmax/Initializer/truncated_normal/TruncatedNormal\n",
      "softmax/W_softmax/Initializer/truncated_normal/mul\n",
      "softmax/W_softmax/Initializer/truncated_normal\n",
      "softmax/W_softmax\n",
      "softmax/W_softmax/Assign\n",
      "softmax/W_softmax/read\n",
      "softmax/b_softmax/Initializer/Const\n",
      "softmax/b_softmax\n",
      "softmax/b_softmax/Assign\n",
      "softmax/b_softmax/read\n",
      "softmax/MatMul\n",
      "softmax/add\n",
      "softmax/softmax\n",
      "Loss/Log\n",
      "Loss/mul\n",
      "Loss/Const\n",
      "Loss/Sum\n",
      "Loss/Neg\n",
      "gradients_1/Shape\n",
      "gradients_1/grad_ys_0\n",
      "gradients_1/Fill\n",
      "gradients_1/Loss/Neg_grad/Neg\n",
      "gradients_1/Loss/Sum_grad/Reshape/shape\n",
      "gradients_1/Loss/Sum_grad/Reshape\n",
      "gradients_1/Loss/Sum_grad/Shape\n",
      "gradients_1/Loss/Sum_grad/Tile\n",
      "gradients_1/Loss/mul_grad/Shape\n",
      "gradients_1/Loss/mul_grad/Shape_1\n",
      "gradients_1/Loss/mul_grad/BroadcastGradientArgs\n",
      "gradients_1/Loss/mul_grad/Mul\n",
      "gradients_1/Loss/mul_grad/Sum\n",
      "gradients_1/Loss/mul_grad/Reshape\n",
      "gradients_1/Loss/mul_grad/Mul_1\n",
      "gradients_1/Loss/mul_grad/Sum_1\n",
      "gradients_1/Loss/mul_grad/Reshape_1\n",
      "gradients_1/Loss/mul_grad/tuple/group_deps\n",
      "gradients_1/Loss/mul_grad/tuple/control_dependency\n",
      "gradients_1/Loss/mul_grad/tuple/control_dependency_1\n",
      "gradients_1/Loss/Log_grad/Reciprocal\n",
      "gradients_1/Loss/Log_grad/mul\n",
      "gradients_1/softmax/softmax_grad/mul\n",
      "gradients_1/softmax/softmax_grad/Sum/reduction_indices\n",
      "gradients_1/softmax/softmax_grad/Sum\n",
      "gradients_1/softmax/softmax_grad/sub\n",
      "gradients_1/softmax/softmax_grad/mul_1\n",
      "gradients_1/softmax/add_grad/Shape\n",
      "gradients_1/softmax/add_grad/Shape_1\n",
      "gradients_1/softmax/add_grad/BroadcastGradientArgs\n",
      "gradients_1/softmax/add_grad/Sum\n",
      "gradients_1/softmax/add_grad/Reshape\n",
      "gradients_1/softmax/add_grad/Sum_1\n",
      "gradients_1/softmax/add_grad/Reshape_1\n",
      "gradients_1/softmax/add_grad/tuple/group_deps\n",
      "gradients_1/softmax/add_grad/tuple/control_dependency\n",
      "gradients_1/softmax/add_grad/tuple/control_dependency_1\n",
      "gradients_1/softmax/MatMul_grad/MatMul\n",
      "gradients_1/softmax/MatMul_grad/MatMul_1\n",
      "gradients_1/softmax/MatMul_grad/tuple/group_deps\n",
      "gradients_1/softmax/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/softmax/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/fc1/dropout/mul_1_grad/Shape\n",
      "gradients_1/fc1/dropout/mul_1_grad/Shape_1\n",
      "gradients_1/fc1/dropout/mul_1_grad/BroadcastGradientArgs\n",
      "gradients_1/fc1/dropout/mul_1_grad/Mul\n",
      "gradients_1/fc1/dropout/mul_1_grad/Sum\n",
      "gradients_1/fc1/dropout/mul_1_grad/Reshape\n",
      "gradients_1/fc1/dropout/mul_1_grad/Mul_1\n",
      "gradients_1/fc1/dropout/mul_1_grad/Sum_1\n",
      "gradients_1/fc1/dropout/mul_1_grad/Reshape_1\n",
      "gradients_1/fc1/dropout/mul_1_grad/tuple/group_deps\n",
      "gradients_1/fc1/dropout/mul_1_grad/tuple/control_dependency\n",
      "gradients_1/fc1/dropout/mul_1_grad/tuple/control_dependency_1\n",
      "gradients_1/fc1/dropout/mul_grad/Shape\n",
      "gradients_1/fc1/dropout/mul_grad/Shape_1\n",
      "gradients_1/fc1/dropout/mul_grad/BroadcastGradientArgs\n",
      "gradients_1/fc1/dropout/mul_grad/Mul\n",
      "gradients_1/fc1/dropout/mul_grad/Sum\n",
      "gradients_1/fc1/dropout/mul_grad/Reshape\n",
      "gradients_1/fc1/dropout/mul_grad/Mul_1\n",
      "gradients_1/fc1/dropout/mul_grad/Sum_1\n",
      "gradients_1/fc1/dropout/mul_grad/Reshape_1\n",
      "gradients_1/fc1/dropout/mul_grad/tuple/group_deps\n",
      "gradients_1/fc1/dropout/mul_grad/tuple/control_dependency\n",
      "gradients_1/fc1/dropout/mul_grad/tuple/control_dependency_1\n",
      "gradients_1/fc1/fc1_grad/ReluGrad\n",
      "gradients_1/fc1/add_grad/Shape\n",
      "gradients_1/fc1/add_grad/Shape_1\n",
      "gradients_1/fc1/add_grad/BroadcastGradientArgs\n",
      "gradients_1/fc1/add_grad/Sum\n",
      "gradients_1/fc1/add_grad/Reshape\n",
      "gradients_1/fc1/add_grad/Sum_1\n",
      "gradients_1/fc1/add_grad/Reshape_1\n",
      "gradients_1/fc1/add_grad/tuple/group_deps\n",
      "gradients_1/fc1/add_grad/tuple/control_dependency\n",
      "gradients_1/fc1/add_grad/tuple/control_dependency_1\n",
      "gradients_1/fc1/MatMul_grad/MatMul\n",
      "gradients_1/fc1/MatMul_grad/MatMul_1\n",
      "gradients_1/fc1/MatMul_grad/tuple/group_deps\n",
      "gradients_1/fc1/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/fc1/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/fc1/Reshape_grad/Shape\n",
      "gradients_1/fc1/Reshape_grad/Reshape\n",
      "gradients_1/pool2/pool2_grad/MaxPoolGrad\n",
      "gradients_1/conv2/conv2_grad/ReluGrad\n",
      "gradients_1/conv2/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/conv2/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/conv2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/conv2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/conv2/Conv2D_grad/ShapeN\n",
      "gradients_1/conv2/Conv2D_grad/Conv2DBackpropInput\n",
      "gradients_1/conv2/Conv2D_grad/Conv2DBackpropFilter\n",
      "gradients_1/conv2/Conv2D_grad/tuple/group_deps\n",
      "gradients_1/conv2/Conv2D_grad/tuple/control_dependency\n",
      "gradients_1/conv2/Conv2D_grad/tuple/control_dependency_1\n",
      "gradients_1/pool1/pool1_grad/MaxPoolGrad\n",
      "gradients_1/conv1/conv1_grad/ReluGrad\n",
      "gradients_1/conv1/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/conv1/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/conv1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/conv1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/conv1/Conv2D_grad/ShapeN\n",
      "gradients_1/conv1/Conv2D_grad/Conv2DBackpropInput\n",
      "gradients_1/conv1/Conv2D_grad/Conv2DBackpropFilter\n",
      "gradients_1/conv1/Conv2D_grad/tuple/group_deps\n",
      "gradients_1/conv1/Conv2D_grad/tuple/control_dependency\n",
      "gradients_1/conv1/Conv2D_grad/tuple/control_dependency_1\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "conv1/W_conv1/Adam/Initializer/zeros\n",
      "conv1/W_conv1/Adam\n",
      "conv1/W_conv1/Adam/Assign\n",
      "conv1/W_conv1/Adam/read\n",
      "conv1/W_conv1/Adam_1/Initializer/zeros\n",
      "conv1/W_conv1/Adam_1\n",
      "conv1/W_conv1/Adam_1/Assign\n",
      "conv1/W_conv1/Adam_1/read\n",
      "conv1/b_conv1/Adam/Initializer/zeros\n",
      "conv1/b_conv1/Adam\n",
      "conv1/b_conv1/Adam/Assign\n",
      "conv1/b_conv1/Adam/read\n",
      "conv1/b_conv1/Adam_1/Initializer/zeros\n",
      "conv1/b_conv1/Adam_1\n",
      "conv1/b_conv1/Adam_1/Assign\n",
      "conv1/b_conv1/Adam_1/read\n",
      "conv2/W_conv2/Adam/Initializer/zeros/shape_as_tensor\n",
      "conv2/W_conv2/Adam/Initializer/zeros/Const\n",
      "conv2/W_conv2/Adam/Initializer/zeros\n",
      "conv2/W_conv2/Adam\n",
      "conv2/W_conv2/Adam/Assign\n",
      "conv2/W_conv2/Adam/read\n",
      "conv2/W_conv2/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "conv2/W_conv2/Adam_1/Initializer/zeros/Const\n",
      "conv2/W_conv2/Adam_1/Initializer/zeros\n",
      "conv2/W_conv2/Adam_1\n",
      "conv2/W_conv2/Adam_1/Assign\n",
      "conv2/W_conv2/Adam_1/read\n",
      "conv2/b_conv2/Adam/Initializer/zeros\n",
      "conv2/b_conv2/Adam\n",
      "conv2/b_conv2/Adam/Assign\n",
      "conv2/b_conv2/Adam/read\n",
      "conv2/b_conv2/Adam_1/Initializer/zeros\n",
      "conv2/b_conv2/Adam_1\n",
      "conv2/b_conv2/Adam_1/Assign\n",
      "conv2/b_conv2/Adam_1/read\n",
      "fc1/W_fc1/Adam/Initializer/zeros/shape_as_tensor\n",
      "fc1/W_fc1/Adam/Initializer/zeros/Const\n",
      "fc1/W_fc1/Adam/Initializer/zeros\n",
      "fc1/W_fc1/Adam\n",
      "fc1/W_fc1/Adam/Assign\n",
      "fc1/W_fc1/Adam/read\n",
      "fc1/W_fc1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "fc1/W_fc1/Adam_1/Initializer/zeros/Const\n",
      "fc1/W_fc1/Adam_1/Initializer/zeros\n",
      "fc1/W_fc1/Adam_1\n",
      "fc1/W_fc1/Adam_1/Assign\n",
      "fc1/W_fc1/Adam_1/read\n",
      "fc1/b_fc1/Adam/Initializer/zeros/shape_as_tensor\n",
      "fc1/b_fc1/Adam/Initializer/zeros/Const\n",
      "fc1/b_fc1/Adam/Initializer/zeros\n",
      "fc1/b_fc1/Adam\n",
      "fc1/b_fc1/Adam/Assign\n",
      "fc1/b_fc1/Adam/read\n",
      "fc1/b_fc1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "fc1/b_fc1/Adam_1/Initializer/zeros/Const\n",
      "fc1/b_fc1/Adam_1/Initializer/zeros\n",
      "fc1/b_fc1/Adam_1\n",
      "fc1/b_fc1/Adam_1/Assign\n",
      "fc1/b_fc1/Adam_1/read\n",
      "softmax/W_softmax/Adam/Initializer/zeros/shape_as_tensor\n",
      "softmax/W_softmax/Adam/Initializer/zeros/Const\n",
      "softmax/W_softmax/Adam/Initializer/zeros\n",
      "softmax/W_softmax/Adam\n",
      "softmax/W_softmax/Adam/Assign\n",
      "softmax/W_softmax/Adam/read\n",
      "softmax/W_softmax/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "softmax/W_softmax/Adam_1/Initializer/zeros/Const\n",
      "softmax/W_softmax/Adam_1/Initializer/zeros\n",
      "softmax/W_softmax/Adam_1\n",
      "softmax/W_softmax/Adam_1/Assign\n",
      "softmax/W_softmax/Adam_1/read\n",
      "softmax/b_softmax/Adam/Initializer/zeros\n",
      "softmax/b_softmax/Adam\n",
      "softmax/b_softmax/Adam/Assign\n",
      "softmax/b_softmax/Adam/read\n",
      "softmax/b_softmax/Adam_1/Initializer/zeros\n",
      "softmax/b_softmax/Adam_1\n",
      "softmax/b_softmax/Adam_1/Assign\n",
      "softmax/b_softmax/Adam_1/read\n",
      "Adam/learning_rate\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_conv1/W_conv1/ApplyAdam\n",
      "Adam/update_conv1/b_conv1/ApplyAdam\n",
      "Adam/update_conv2/W_conv2/ApplyAdam\n",
      "Adam/update_conv2/b_conv2/ApplyAdam\n",
      "Adam/update_fc1/W_fc1/ApplyAdam\n",
      "Adam/update_fc1/b_fc1/ApplyAdam\n",
      "Adam/update_softmax/W_softmax/ApplyAdam\n",
      "Adam/update_softmax/b_softmax/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "Accuracy/ArgMax/dimension\n",
      "Accuracy/ArgMax\n",
      "Accuracy/ArgMax_1/dimension\n",
      "Accuracy/ArgMax_1\n",
      "Accuracy/Equal\n",
      "Accuracy/Cast\n",
      "Accuracy/Const\n",
      "Accuracy/Mean\n",
      "gradients_2/Shape\n",
      "gradients_2/grad_ys_0\n",
      "gradients_2/Fill\n",
      "gradients_2/Loss/Neg_grad/Neg\n",
      "gradients_2/Loss/Sum_grad/Reshape/shape\n",
      "gradients_2/Loss/Sum_grad/Reshape\n",
      "gradients_2/Loss/Sum_grad/Shape\n",
      "gradients_2/Loss/Sum_grad/Tile\n",
      "gradients_2/Loss/mul_grad/Shape\n",
      "gradients_2/Loss/mul_grad/Shape_1\n",
      "gradients_2/Loss/mul_grad/BroadcastGradientArgs\n",
      "gradients_2/Loss/mul_grad/Mul\n",
      "gradients_2/Loss/mul_grad/Sum\n",
      "gradients_2/Loss/mul_grad/Reshape\n",
      "gradients_2/Loss/mul_grad/Mul_1\n",
      "gradients_2/Loss/mul_grad/Sum_1\n",
      "gradients_2/Loss/mul_grad/Reshape_1\n",
      "gradients_2/Loss/mul_grad/tuple/group_deps\n",
      "gradients_2/Loss/mul_grad/tuple/control_dependency\n",
      "gradients_2/Loss/mul_grad/tuple/control_dependency_1\n",
      "gradients_2/Loss/Log_grad/Reciprocal\n",
      "gradients_2/Loss/Log_grad/mul\n",
      "gradients_2/softmax/softmax_grad/mul\n",
      "gradients_2/softmax/softmax_grad/Sum/reduction_indices\n",
      "gradients_2/softmax/softmax_grad/Sum\n",
      "gradients_2/softmax/softmax_grad/sub\n",
      "gradients_2/softmax/softmax_grad/mul_1\n",
      "gradients_2/softmax/add_grad/Shape\n",
      "gradients_2/softmax/add_grad/Shape_1\n",
      "gradients_2/softmax/add_grad/BroadcastGradientArgs\n",
      "gradients_2/softmax/add_grad/Sum\n",
      "gradients_2/softmax/add_grad/Reshape\n",
      "gradients_2/softmax/add_grad/Sum_1\n",
      "gradients_2/softmax/add_grad/Reshape_1\n",
      "gradients_2/softmax/add_grad/tuple/group_deps\n",
      "gradients_2/softmax/add_grad/tuple/control_dependency\n",
      "gradients_2/softmax/add_grad/tuple/control_dependency_1\n",
      "gradients_2/softmax/MatMul_grad/MatMul\n",
      "gradients_2/softmax/MatMul_grad/MatMul_1\n",
      "gradients_2/softmax/MatMul_grad/tuple/group_deps\n",
      "gradients_2/softmax/MatMul_grad/tuple/control_dependency\n",
      "gradients_2/softmax/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_2/fc1/dropout/mul_1_grad/Shape\n",
      "gradients_2/fc1/dropout/mul_1_grad/Shape_1\n",
      "gradients_2/fc1/dropout/mul_1_grad/BroadcastGradientArgs\n",
      "gradients_2/fc1/dropout/mul_1_grad/Mul\n",
      "gradients_2/fc1/dropout/mul_1_grad/Sum\n",
      "gradients_2/fc1/dropout/mul_1_grad/Reshape\n",
      "gradients_2/fc1/dropout/mul_1_grad/Mul_1\n",
      "gradients_2/fc1/dropout/mul_1_grad/Sum_1\n",
      "gradients_2/fc1/dropout/mul_1_grad/Reshape_1\n",
      "gradients_2/fc1/dropout/mul_1_grad/tuple/group_deps\n",
      "gradients_2/fc1/dropout/mul_1_grad/tuple/control_dependency\n",
      "gradients_2/fc1/dropout/mul_1_grad/tuple/control_dependency_1\n",
      "gradients_2/fc1/dropout/mul_grad/Shape\n",
      "gradients_2/fc1/dropout/mul_grad/Shape_1\n",
      "gradients_2/fc1/dropout/mul_grad/BroadcastGradientArgs\n",
      "gradients_2/fc1/dropout/mul_grad/Mul\n",
      "gradients_2/fc1/dropout/mul_grad/Sum\n",
      "gradients_2/fc1/dropout/mul_grad/Reshape\n",
      "gradients_2/fc1/dropout/mul_grad/Mul_1\n",
      "gradients_2/fc1/dropout/mul_grad/Sum_1\n",
      "gradients_2/fc1/dropout/mul_grad/Reshape_1\n",
      "gradients_2/fc1/dropout/mul_grad/tuple/group_deps\n",
      "gradients_2/fc1/dropout/mul_grad/tuple/control_dependency\n",
      "gradients_2/fc1/dropout/mul_grad/tuple/control_dependency_1\n",
      "gradients_2/fc1/fc1_grad/ReluGrad\n",
      "gradients_2/fc1/add_grad/Shape\n",
      "gradients_2/fc1/add_grad/Shape_1\n",
      "gradients_2/fc1/add_grad/BroadcastGradientArgs\n",
      "gradients_2/fc1/add_grad/Sum\n",
      "gradients_2/fc1/add_grad/Reshape\n",
      "gradients_2/fc1/add_grad/Sum_1\n",
      "gradients_2/fc1/add_grad/Reshape_1\n",
      "gradients_2/fc1/add_grad/tuple/group_deps\n",
      "gradients_2/fc1/add_grad/tuple/control_dependency\n",
      "gradients_2/fc1/add_grad/tuple/control_dependency_1\n",
      "gradients_2/fc1/MatMul_grad/MatMul\n",
      "gradients_2/fc1/MatMul_grad/MatMul_1\n",
      "gradients_2/fc1/MatMul_grad/tuple/group_deps\n",
      "gradients_2/fc1/MatMul_grad/tuple/control_dependency\n",
      "gradients_2/fc1/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_2/fc1/Reshape_grad/Shape\n",
      "gradients_2/fc1/Reshape_grad/Reshape\n",
      "gradients_2/pool2/pool2_grad/MaxPoolGrad\n",
      "gradients_2/conv2/conv2_grad/ReluGrad\n",
      "gradients_2/conv2/BiasAdd_grad/BiasAddGrad\n",
      "gradients_2/conv2/BiasAdd_grad/tuple/group_deps\n",
      "gradients_2/conv2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_2/conv2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_2/conv2/Conv2D_grad/ShapeN\n",
      "gradients_2/conv2/Conv2D_grad/Conv2DBackpropInput\n",
      "gradients_2/conv2/Conv2D_grad/Conv2DBackpropFilter\n",
      "gradients_2/conv2/Conv2D_grad/tuple/group_deps\n",
      "gradients_2/conv2/Conv2D_grad/tuple/control_dependency\n",
      "gradients_2/conv2/Conv2D_grad/tuple/control_dependency_1\n",
      "gradients_2/pool1/pool1_grad/MaxPoolGrad\n",
      "gradients_2/conv1/conv1_grad/ReluGrad\n",
      "gradients_2/conv1/BiasAdd_grad/BiasAddGrad\n",
      "gradients_2/conv1/BiasAdd_grad/tuple/group_deps\n",
      "gradients_2/conv1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_2/conv1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_2/conv1/Conv2D_grad/ShapeN\n",
      "gradients_2/conv1/Conv2D_grad/Conv2DBackpropInput\n",
      "gradients_2/conv1/Conv2D_grad/Conv2DBackpropFilter\n",
      "gradients_2/conv1/Conv2D_grad/tuple/group_deps\n",
      "gradients_2/conv1/Conv2D_grad/tuple/control_dependency\n",
      "gradients_2/conv1/Conv2D_grad/tuple/control_dependency_1\n",
      "beta1_power_1/initial_value\n",
      "beta1_power_1\n",
      "beta1_power_1/Assign\n",
      "beta1_power_1/read\n",
      "beta2_power_1/initial_value\n",
      "beta2_power_1\n",
      "beta2_power_1/Assign\n",
      "beta2_power_1/read\n",
      "conv1/W_conv1/Adam_2/Initializer/zeros\n",
      "conv1/W_conv1/Adam_2\n",
      "conv1/W_conv1/Adam_2/Assign\n",
      "conv1/W_conv1/Adam_2/read\n",
      "conv1/W_conv1/Adam_3/Initializer/zeros\n",
      "conv1/W_conv1/Adam_3\n",
      "conv1/W_conv1/Adam_3/Assign\n",
      "conv1/W_conv1/Adam_3/read\n",
      "conv1/b_conv1/Adam_2/Initializer/zeros\n",
      "conv1/b_conv1/Adam_2\n",
      "conv1/b_conv1/Adam_2/Assign\n",
      "conv1/b_conv1/Adam_2/read\n",
      "conv1/b_conv1/Adam_3/Initializer/zeros\n",
      "conv1/b_conv1/Adam_3\n",
      "conv1/b_conv1/Adam_3/Assign\n",
      "conv1/b_conv1/Adam_3/read\n",
      "conv2/W_conv2/Adam_2/Initializer/zeros/shape_as_tensor\n",
      "conv2/W_conv2/Adam_2/Initializer/zeros/Const\n",
      "conv2/W_conv2/Adam_2/Initializer/zeros\n",
      "conv2/W_conv2/Adam_2\n",
      "conv2/W_conv2/Adam_2/Assign\n",
      "conv2/W_conv2/Adam_2/read\n",
      "conv2/W_conv2/Adam_3/Initializer/zeros/shape_as_tensor\n",
      "conv2/W_conv2/Adam_3/Initializer/zeros/Const\n",
      "conv2/W_conv2/Adam_3/Initializer/zeros\n",
      "conv2/W_conv2/Adam_3\n",
      "conv2/W_conv2/Adam_3/Assign\n",
      "conv2/W_conv2/Adam_3/read\n",
      "conv2/b_conv2/Adam_2/Initializer/zeros\n",
      "conv2/b_conv2/Adam_2\n",
      "conv2/b_conv2/Adam_2/Assign\n",
      "conv2/b_conv2/Adam_2/read\n",
      "conv2/b_conv2/Adam_3/Initializer/zeros\n",
      "conv2/b_conv2/Adam_3\n",
      "conv2/b_conv2/Adam_3/Assign\n",
      "conv2/b_conv2/Adam_3/read\n",
      "fc1/W_fc1/Adam_2/Initializer/zeros/shape_as_tensor\n",
      "fc1/W_fc1/Adam_2/Initializer/zeros/Const\n",
      "fc1/W_fc1/Adam_2/Initializer/zeros\n",
      "fc1/W_fc1/Adam_2\n",
      "fc1/W_fc1/Adam_2/Assign\n",
      "fc1/W_fc1/Adam_2/read\n",
      "fc1/W_fc1/Adam_3/Initializer/zeros/shape_as_tensor\n",
      "fc1/W_fc1/Adam_3/Initializer/zeros/Const\n",
      "fc1/W_fc1/Adam_3/Initializer/zeros\n",
      "fc1/W_fc1/Adam_3\n",
      "fc1/W_fc1/Adam_3/Assign\n",
      "fc1/W_fc1/Adam_3/read\n",
      "fc1/b_fc1/Adam_2/Initializer/zeros/shape_as_tensor\n",
      "fc1/b_fc1/Adam_2/Initializer/zeros/Const\n",
      "fc1/b_fc1/Adam_2/Initializer/zeros\n",
      "fc1/b_fc1/Adam_2\n",
      "fc1/b_fc1/Adam_2/Assign\n",
      "fc1/b_fc1/Adam_2/read\n",
      "fc1/b_fc1/Adam_3/Initializer/zeros/shape_as_tensor\n",
      "fc1/b_fc1/Adam_3/Initializer/zeros/Const\n",
      "fc1/b_fc1/Adam_3/Initializer/zeros\n",
      "fc1/b_fc1/Adam_3\n",
      "fc1/b_fc1/Adam_3/Assign\n",
      "fc1/b_fc1/Adam_3/read\n",
      "softmax/W_softmax/Adam_2/Initializer/zeros/shape_as_tensor\n",
      "softmax/W_softmax/Adam_2/Initializer/zeros/Const\n",
      "softmax/W_softmax/Adam_2/Initializer/zeros\n",
      "softmax/W_softmax/Adam_2\n",
      "softmax/W_softmax/Adam_2/Assign\n",
      "softmax/W_softmax/Adam_2/read\n",
      "softmax/W_softmax/Adam_3/Initializer/zeros/shape_as_tensor\n",
      "softmax/W_softmax/Adam_3/Initializer/zeros/Const\n",
      "softmax/W_softmax/Adam_3/Initializer/zeros\n",
      "softmax/W_softmax/Adam_3\n",
      "softmax/W_softmax/Adam_3/Assign\n",
      "softmax/W_softmax/Adam_3/read\n",
      "softmax/b_softmax/Adam_2/Initializer/zeros\n",
      "softmax/b_softmax/Adam_2\n",
      "softmax/b_softmax/Adam_2/Assign\n",
      "softmax/b_softmax/Adam_2/read\n",
      "softmax/b_softmax/Adam_3/Initializer/zeros\n",
      "softmax/b_softmax/Adam_3\n",
      "softmax/b_softmax/Adam_3/Assign\n",
      "softmax/b_softmax/Adam_3/read\n",
      "Adam_1/learning_rate\n",
      "Adam_1/beta1\n",
      "Adam_1/beta2\n",
      "Adam_1/epsilon\n",
      "Adam_1/update_conv1/W_conv1/ApplyAdam\n",
      "Adam_1/update_conv1/b_conv1/ApplyAdam\n",
      "Adam_1/update_conv2/W_conv2/ApplyAdam\n",
      "Adam_1/update_conv2/b_conv2/ApplyAdam\n",
      "Adam_1/update_fc1/W_fc1/ApplyAdam\n",
      "Adam_1/update_fc1/b_fc1/ApplyAdam\n",
      "Adam_1/update_softmax/W_softmax/ApplyAdam\n",
      "Adam_1/update_softmax/b_softmax/ApplyAdam\n",
      "Adam_1/mul\n",
      "Adam_1/Assign\n",
      "Adam_1/mul_1\n",
      "Adam_1/Assign_1\n",
      "Adam_1\n",
      "init_2\n",
      "save_1/filename/input\n",
      "save_1/filename\n",
      "save_1/Const\n",
      "save_1/SaveV2/tensor_names\n",
      "save_1/SaveV2/shape_and_slices\n",
      "save_1/SaveV2\n",
      "save_1/control_dependency\n",
      "save_1/RestoreV2/tensor_names\n",
      "save_1/RestoreV2/shape_and_slices\n",
      "save_1/RestoreV2\n",
      "save_1/Assign\n",
      "save_1/Assign_1\n",
      "save_1/Assign_2\n",
      "save_1/Assign_3\n",
      "save_1/Assign_4\n",
      "save_1/Assign_5\n",
      "save_1/Assign_6\n",
      "save_1/Assign_7\n",
      "save_1/Assign_8\n",
      "save_1/Assign_9\n",
      "save_1/Assign_10\n",
      "save_1/Assign_11\n",
      "save_1/Assign_12\n",
      "save_1/Assign_13\n",
      "save_1/Assign_14\n",
      "save_1/Assign_15\n",
      "save_1/Assign_16\n",
      "save_1/Assign_17\n",
      "save_1/Assign_18\n",
      "save_1/Assign_19\n",
      "save_1/Assign_20\n",
      "save_1/Assign_21\n",
      "save_1/Assign_22\n",
      "save_1/Assign_23\n",
      "save_1/Assign_24\n",
      "save_1/Assign_25\n",
      "save_1/Assign_26\n",
      "save_1/Assign_27\n",
      "save_1/Assign_28\n",
      "save_1/Assign_29\n",
      "save_1/Assign_30\n",
      "save_1/Assign_31\n",
      "save_1/Assign_32\n",
      "save_1/Assign_33\n",
      "save_1/Assign_34\n",
      "save_1/Assign_35\n",
      "save_1/Assign_36\n",
      "save_1/Assign_37\n",
      "save_1/Assign_38\n",
      "save_1/Assign_39\n",
      "save_1/Assign_40\n",
      "save_1/Assign_41\n",
      "save_1/Assign_42\n",
      "save_1/Assign_43\n",
      "save_1/Assign_44\n",
      "save_1/Assign_45\n",
      "save_1/Assign_46\n",
      "save_1/Assign_47\n",
      "save_1/restore_all\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow——基于BP模型和MNIST数据集的手写数字识别\n",
    "import tensorflow as tf\n",
    "# 调用Tensorflow中MNIST(手写数字）数据集\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "# 定义占位符，转换长784向量为28*28*1\n",
    "with tf.variable_scope('Placeholder'):\n",
    "    x = tf.placeholder(\"float\", name='X_placeholder', shape=[None, 784])\n",
    "    y_ = tf.placeholder(\"float\", name='y_placeholder', shape=[None, 10])\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# 两个卷积层、两个池化层、一个全连接层和一个softmax层。Softmax层用于分类\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 1, 32], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    b_conv1 = tf.get_variable('b_conv1', shape = [32], initializer=tf.constant_initializer(0.1))\n",
    "    conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    pre_activation = tf.nn.bias_add(conv1, b_conv1)\n",
    "    activation = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    pool1 = tf.nn.max_pool(activation, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=scope.name)\n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, 32, 64], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    b_conv2 = tf.get_variable('b_conv2', shape=[64], initializer=tf.constant_initializer(0.1))\n",
    "    conv2 = tf.nn.conv2d(pool1, W_conv2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    pre_activation = tf.nn.bias_add(conv2, b_conv2)\n",
    "    activation = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    pool2 = tf.nn.max_pool(activation, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=scope.name)\n",
    "\n",
    "with tf.variable_scope('fc1') as scope:\n",
    "    W_fc1 = tf.get_variable(\"W_fc1\", shape=[7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "    pool2_falt = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    fc1 = tf.matmul(pool2_falt, W_fc1) + b_fc1\n",
    "    activation = tf.nn.relu(fc1, name=scope.name)\n",
    "    drop_fc1 = tf.nn.dropout(activation, keep_prob=0.5)\n",
    "\n",
    "with tf.variable_scope('softmax') as scope:\n",
    "    W_softmax = tf.get_variable(\"W_softmax\", shape=[1024, 10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_softmax = tf.get_variable(\"b_softmax\", shape=[10], initializer=tf.constant_initializer(0.1))\n",
    "    y_conv = tf.nn.softmax(tf.matmul(drop_fc1, W_softmax) + b_softmax, name=scope.name)\n",
    "\n",
    "# 损失函数是目标类别和预测类别之间的交叉熵\n",
    "with tf.variable_scope('Loss'):\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 计算准确率\n",
    "'''tf.argmax()函数：是返回对象在某一维上的其数据最大值所对应的索引值，由于这里的标签向量都是由0,1组成，因此最大值1所在的索引位置就是对应的类别标签\n",
    "tf.argmax(y_conv,1)返回的是对于任一输入x预测到的标签值，tf.argmax(y_,1)代表正确的标签值\n",
    "correct_prediction 返回一个布尔数组。为计算分类准确率，将布尔值转换为浮点数来代表对与错，然后取平均值。如[True, False, True, True]变为[1,0,1,1]，计算出准确率就为0.75。'''\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # batch是从MNIST数据集中按批次数取得的：数据项与标签项feed_dict=({x:batch[0],y_:batch[1]}语句：是将batch[0]、batch[1]代表的值传入x、y_。\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "    # 模型保存\n",
    "    saver = tf.train.Saver()\n",
    "    last_chkp = saver.save(sess, 'results/graph.chkp')\n",
    "    # sv.saver.save(sess, 'results/graph.chkp')\n",
    "\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:23:54.578801  9420 deprecation.py:323] From <ipython-input-5-887d3724c5de>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0802 14:23:54.580289  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0802 14:23:54.581776  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:23:54.850614  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0802 14:23:54.852592  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0802 14:23:54.918068  9420 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "W0802 14:23:55.271748  9420 deprecation.py:506] From <ipython-input-5-887d3724c5de>:49: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.18\n",
      "step 500, training accuracy 0.92\n",
      "step 1000, training accuracy 0.98\n",
      "step 1500, training accuracy 0.92\n",
      "test accuracy 0.9763\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow——基于LeNet模型和MNIST数据集的手写数字识别\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# 将图片变为4d张量，28*28，第四维对应图片通道数，灰度为1，彩色为3\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "# 第一层卷积，卷积核5*5，输入通道1，输出通道32个特征图。\n",
    "# 将x_image与权重张量进行卷积，加偏置项，使用 ReLU激活函数，最后max_pool_2x2最大池化操作将图片减小到14*14。\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# 第二层卷积\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# 全连接层，1024个神经元全连接，用于处理整张图像\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# 输出层\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "# 训练评估\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))   # 交叉熵损失\n",
    "# 定义优化策略：采用Adam优化算法，以0.0001的学习率进行最小化损失操作\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 判断预测值与标签是否相等，返回一个布尔值\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  # 将布尔值转化为数值，计算平均准确率\n",
    "sess.run(tf.initialize_all_variables())  # 变量初始化\n",
    "for i in range(2000):             # 在循环中实现参数更新\n",
    "    batch = mnist.train.next_batch(50)  #batch size为50，每次获取50张数据\n",
    "    if i%500 == 0:              # 500次在屏幕打印一次信息\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})   # 将数据填入占位符，喂给网络\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))  # 输出迭代次数和测试训练的准确率\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:46:40.418022  9420 deprecation_wrapper.py:119] From c:\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0802 14:46:40.421798  9420 deprecation_wrapper.py:119] From c:\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 14:46:40.430186  9420 deprecation_wrapper.py:119] From c:\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update count:  3763\n",
      "Success Rate:  96.0 %\n",
      "All input:  100  failed_count:  4\n",
      "T1: w: [[-155.19219219   68.61561562]]  b: [[-619]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:129: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow——单层感知机和多层感知机的实现\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "class TrainDataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def GenerateRandomData(self, count, grandient, offset):\n",
    "        x1 = np.linspace(1, 5, count)\n",
    "        x2 = grandient * x1 + np.random.randint(-10, 10, *x1.shape) + offset\n",
    "        dataset = []\n",
    "        y = []\n",
    "        for i in range(*x1.shape):\n",
    "            dataset.append([x1[i], x2[i]])\n",
    "            real_value = grandient * x1[i] + offset\n",
    "            if real_value > x2[i]:\n",
    "                y.append(-1)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        return x1, x2, np.mat(y), np.mat(dataset)\n",
    "\n",
    "\n",
    "class SimplePerceptron:\n",
    "    def __init__(self, train_data=[], real_result=[], eta=1):\n",
    "        self.w = np.zeros([1, len(train_data.T)], int)\n",
    "        self.b = 0\n",
    "        self.eta = eta\n",
    "        self.train_data = train_data\n",
    "        self.real_result = real_result\n",
    "\n",
    "    def nomalize(self, x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def model(self, x):\n",
    "        # Here are matrix dot multiply get one value\n",
    "        y = np.dot(x, self.w.T) + self.b\n",
    "        # Use sign to nomalize the result\n",
    "        predict_v = self.nomalize(y)\n",
    "        return predict_v, y\n",
    "\n",
    "    def update(self, x, y):\n",
    "        # w = w + n*y_i*x_i\n",
    "        self.w = self.w + self.eta * y * x\n",
    "        # b = b + n*y_i\n",
    "        self.b = self.b + self.eta * y\n",
    "\n",
    "    def loss(slef, fx, y):\n",
    "        return fx.astype(int) * y\n",
    "    \n",
    "    def train(self, count):\n",
    "        update_count = 0\n",
    "        while count > 0:\n",
    "            # count--\n",
    "            count = count - 1\n",
    "            if len(self.train_data) <= 0:\n",
    "                print(\"exception exit\")\n",
    "                break\n",
    "            # random select one train data\n",
    "            index = np.random.randint(0, len(self.train_data) - 1)\n",
    "            x = self.train_data[index]\n",
    "            y = self.real_result.T[index]\n",
    "            # wx+b\n",
    "            predict_v, linear_y_v = self.model(x)\n",
    "            # y_i*(wx+b) > 0, the classify is correct, else it's error\n",
    "            if self.loss(y, linear_y_v) > 0:\n",
    "                continue\n",
    "            update_count = update_count + 1\n",
    "            self.update(x, y)\n",
    "        print(\"update count: \", update_count)\n",
    "        pass\n",
    "\n",
    "    def verify(self, verify_data, verify_result):\n",
    "        size = len(verify_data)\n",
    "        failed_count = 0\n",
    "        if size <= 0:\n",
    "            pass\n",
    "        for i in range(size):\n",
    "            x = verify_data[i]\n",
    "            y = verify_result.T[i]\n",
    "            if self.loss(y, self.model(x)[1]) > 0:\n",
    "                continue\n",
    "            failed_count = failed_count + 1\n",
    "        success_rate = (1.0 - (float(failed_count) / size)) * 100\n",
    "        print(\"Success Rate: \", success_rate, \"%\")\n",
    "        print(\"All input: \", size, \" failed_count: \", failed_count)\n",
    "\n",
    "    def predict(self, predict_data):\n",
    "        size = len(predict_data)\n",
    "        result = []\n",
    "        if size <= 0:\n",
    "            pass\n",
    "        for i in range(size):\n",
    "            x = verify_data[i]\n",
    "            y = verify_result.T[i]\n",
    "            result.append(self.model(x)[0])\n",
    "        return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Init some parameters\n",
    "    gradient = 2\n",
    "    offset   = 10\n",
    "    point_num = 1000\n",
    "    train_num = 50000\n",
    "    loader = TrainDataLoader()\n",
    "    x, y, result, train_data =  loader.GenerateRandomData(point_num, gradient, offset)\n",
    "    x_t, y_t, test_real_result, test_data =  loader.GenerateRandomData(100, gradient, offset)\n",
    "\n",
    "    # First training\n",
    "    perceptron = SimplePerceptron(train_data, result)\n",
    "    perceptron.train(train_num)\n",
    "    perceptron.verify(test_data, test_real_result)\n",
    "    print(\"T1: w:\", perceptron.w,\" b:\", perceptron.b)\n",
    "\n",
    "    # Draw the figure\n",
    "    # 1. draw the (x,y) points\n",
    "    plt.plot(x, y, \"*\", color='gray')\n",
    "    plt.plot(x_t, y_t, \"+\")\n",
    "    # 2. draw y=gradient*x+offset line\n",
    "    plt.plot(x,x.dot(gradient)+offset, color=\"red\")\n",
    "    # 3. draw the line w_1*x_1 + w_2*x_2 + b = 0\n",
    "    plt.plot(x, -(x.dot(float(perceptron.w.T[0]))+float(perceptron.b))/float(perceptron.w.T[1]), color='green')\n",
    "    plt.show()\n",
    "    plt.savefig('result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 14:50:51.063311 13672 deprecation.py:323] From <ipython-input-2-77cf559f773b>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0805 14:50:51.064260 13672 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0805 14:50:51.068758 13672 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 14:50:51.372771 13672 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0805 14:50:51.375253 13672 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0805 14:50:51.444199 13672 deprecation.py:323] From c:\\python3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 14:50:51.834077 13672 deprecation.py:506] From <ipython-input-2-77cf559f773b>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0805 14:50:51.948346 13672 deprecation.py:506] From c:\\python3\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n",
      "Epoch 1: cost is 393.6096366,accuracy is 0.9595467.\n",
      "Train Finished!\n",
      "Test accuracy is 0.9790.\n",
      "Label: [2]\n",
      "Prediction: [2]\n"
     ]
    }
   ],
   "source": [
    "# 多层感知机\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# 0，导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(mnist.train.images.shape, mnist.train.labels.shape)\n",
    "\n",
    "# 1，定义模型计算公式\n",
    "sess = tf.InteractiveSession()\n",
    "in_units = 784\n",
    "h1_units = 300\n",
    "W1 = tf.Variable(tf.truncated_normal([in_units, h1_units], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([h1_units]))\n",
    "W2 = tf.Variable(tf.zeros([h1_units, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, in_units])\n",
    "keep_prob = tf.placeholder(tf.float32, )\n",
    "h1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)\n",
    "y_pred = tf.nn.softmax(tf.matmul(h1_drop, W2) + b2)\n",
    "\n",
    "# 2,定义loss，选定优化器\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate=0.3).minimize(cross_entropy)\n",
    "\n",
    "# 3,定义精确度计算公式\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 4,初始化参数\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# 4,迭代训练\n",
    "epoch_num = 1\n",
    "for epoch in range(epoch_num):\n",
    "    avg_accuracy = 0.0\n",
    "    avg_cost = 0.0\n",
    "    for i in range(3000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        cost, acc, _ = sess.run([cross_entropy, accuracy, train_step],\n",
    "                                feed_dict={X: batch_xs, y: batch_ys, keep_prob: 0.75})\n",
    "        avg_cost += cost\n",
    "        avg_accuracy += acc / 3000\n",
    "    print('Epoch %d: cost is %.7f,accuracy is %.7f.' % (epoch + 1, avg_cost, avg_accuracy))\n",
    "print('Train Finished!')\n",
    "print('Test accuracy is %.4f.' % accuracy.eval({X: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "# 5,Get one and predict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\", sess.run(tf.argmax(y_pred, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1.0}))\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "plt.savefig('result1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow——基于DNN模型和Iris data set的鸢尾花品种识别\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from six.moves.urllib.request import urlopen\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(IRIS_TRAINING):\n",
    "        raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "        with open(IRIS_TRAINING, 'wb') as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    if not os.path.exists(IRIS_TEST):\n",
    "        raw = urlopen(IRIS_TEST_URL).read()\n",
    "        with open(IRIS_TEST, 'wb') as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    # Load datasets.\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)\n",
    "    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)\n",
    "\n",
    "    # Specify that all features have real-value data\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n",
    "\n",
    "    # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3, model_dir=\"/iris_model\")\n",
    "\n",
    "    # Define the training inputs\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(training_set.data)}, y=np.array(training_set.target), num_epochs=None, shuffle=True)\n",
    "\n",
    "    # Train model.\n",
    "    classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "\n",
    "    # Define the test inputs\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(test_set.data)}, y=np.array(test_set.target), num_epochs=1, shuffle=False)\n",
    "\n",
    "    # Evaluate accuracy.\n",
    "    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "    print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    "    # Classify two new flower samples.\n",
    "    new_samples = np.array([[6.4, 3.2, 4.5, 1.5],[5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": new_samples}, num_epochs=1, shuffle=False)\n",
    "\n",
    "    predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "    predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "    print(\"New Samples, Class Predictions:    {}\\n\".format(predicted_classes))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow——基于Time Series的时间序列预测\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from os import path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\n",
    "from tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n",
    "from tensorflow.contrib.timeseries.python.timeseries import NumpyReader\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"agg\")\n",
    "\n",
    "\n",
    "x = np.array(range(1000))\n",
    "noise = np.random.uniform(-0.2, 0.2, 1000)\n",
    "y = np.sin(np.pi * x / 50) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise\n",
    "\n",
    "plt.plot(x, y, 'g-', label='Input data')\n",
    "plt.legend()\n",
    "# plt.savefig('Input_timeseries_data.jpg')\n",
    "\n",
    "data = {\n",
    "    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n",
    "    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n",
    "}\n",
    "\n",
    "# reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n",
    "reader = NumpyReader(data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    full_data = reader.read_full()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # print(sess.run(full_data))\n",
    "    coord.request_stop()\n",
    "\n",
    "train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=2, window_size=10)\n",
    "with tf.Session() as sess:\n",
    "    batch_data = train_input_fn.create_batch()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    one_batch = sess.run(batch_data[0])\n",
    "    coord.request_stop()\n",
    "print('one_batch_data:', one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\n",
    "from tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n",
    "from tensorflow.contrib.timeseries.python.timeseries import NumpyReader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class _LSTMModel(ts_model.SequentialTimeSeriesModel):\n",
    "  \"\"\"使用RNN构建一个时间序列模型\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, num_features, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    初始化模型\n",
    "    模型输入:\n",
    "      num_units:模型中的LSTM单元数量.\n",
    "      num_features: 时间序列的维度（每次迭代产生的特征数）.\n",
    "      dtype: 要使用的浮点数据类型.\n",
    "    \"\"\"\n",
    "    super(_LSTMModel, self).__init__(\n",
    "      \n",
    "        train_output_names=[\"mean\"],\n",
    "        predict_output_names=[\"mean\"],\n",
    "        num_features=num_features,\n",
    "        dtype=dtype)\n",
    "    self._num_units = num_units\n",
    "   \n",
    "    self._lstm_cell = None\n",
    "    self._lstm_cell_run = None\n",
    "    self._predict_from_lstm_output = None\n",
    "\n",
    "  def initialize_graph(self, input_statistics):\n",
    "    \"\"\"\n",
    "    保存组件模型，可以重复使用。\n",
    "    每次创建新图形时都会调用此方法。\n",
    "    \"\"\"\n",
    "    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n",
    "    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n",
    "\n",
    "    self._lstm_cell_run = tf.make_template(\n",
    "        name_=\"lstm_cell\",\n",
    "        func_=self._lstm_cell,\n",
    "        create_scope_now_=True)\n",
    " \n",
    "    self._predict_from_lstm_output = tf.make_template(\n",
    "        name_=\"predict_from_lstm_output\",\n",
    "        func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n",
    "        create_scope_now_=True)\n",
    "\n",
    "  def get_start_state(self):\n",
    "    \"\"\"返回到时间序列模型的初始状态\"\"\"\n",
    "    return (\n",
    "        # 跟踪与此状态相关的时间以进行错误检查.\n",
    "        tf.zeros([], dtype=tf.int64),\n",
    "        # 之前的观察或预测值.\n",
    "        tf.zeros([self.num_features], dtype=self.dtype),\n",
    "        # RNN单元的状态\n",
    "        [tf.squeeze(state_element, axis=0)\n",
    "         for state_element\n",
    "         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n",
    "\n",
    "  def _transform(self, data):\n",
    "    \"\"\"对输入数据归一化处理，以保证稳定训练\"\"\"\n",
    "    mean, variance = self._input_statistics.overall_feature_moments\n",
    "    return (data - mean) / variance\n",
    "\n",
    "  def _de_transform(self, data):\n",
    "    \"\"\"将数据恢复到输入的尺度.\"\"\"\n",
    "    mean, variance = self._input_statistics.overall_feature_moments\n",
    "    return data * variance + mean\n",
    "\n",
    "  def _filtering_step(self, current_times, current_values, state, predictions):\n",
    "    \"\"\"计算损失值，更新模型\n",
    "    输入:\n",
    "      current_times:  一个[batch size]整型张量.\n",
    "      current_values: 一个[batch size, self.num_features] 浮点张量\n",
    "      state: 模型状态元组.\n",
    "      predictions: 上一个预测步骤的输出.\n",
    "    返回:\n",
    "      模型新状态元组和预测字典\n",
    "    \"\"\"\n",
    "    state_from_time, prediction, lstm_state = state\n",
    "    with tf.control_dependencies(\n",
    "            [tf.assert_equal(current_times, state_from_time)]):\n",
    "      transformed_values = self._transform(current_values)\n",
    "      # 使用均方误差计算损失\n",
    "      predictions[\"loss\"] = tf.reduce_mean(\n",
    "          (prediction - transformed_values) ** 2, axis=-1)\n",
    "      \n",
    "      new_state_tuple = (current_times, transformed_values, lstm_state)\n",
    "    return (new_state_tuple, predictions)\n",
    "\n",
    "  def _prediction_step(self, current_times, state):\n",
    "    \"\"\"使用先前的观察或预测来推进RNN状态\"\"\"\n",
    "    _, previous_observation_or_prediction, lstm_state = state\n",
    "    lstm_output, new_lstm_state = self._lstm_cell_run(\n",
    "        inputs=previous_observation_or_prediction, state=lstm_state)\n",
    "    next_prediction = self._predict_from_lstm_output(lstm_output)\n",
    "    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n",
    "    return new_state_tuple, {\"mean\": self._de_transform(next_prediction)}\n",
    "\n",
    "  def _imputation_step(self, current_times, state):\n",
    "    \"\"\"跳过差距，推进模型状态\"\"\"\n",
    "    return state\n",
    "\n",
    "  def _exogenous_input_step(\n",
    "          self, current_times, current_exogenous_regressors, state):\n",
    "    raise NotImplementedError(\n",
    "         \"Exogenous inputs are not implemented for this example.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  #使用np.sin生成实验用的时间序列数据\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  x = np.array(range(1000))\n",
    "  noise = np.random.uniform(-0.2, 0.2, 1000)\n",
    "  y = np.sin(np.pi * x / 50 ) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise\n",
    "\n",
    "  data = {\n",
    "      tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n",
    "      tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n",
    "  }\n",
    "  #读取时间序列数据（可以是numpy数组形式，也可以是csv形式）\n",
    "  reader = NumpyReader(data)\n",
    "  #reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n",
    "\n",
    "  \n",
    "  #在reader的所有数据中，随机选取窗口长度为window_size的序列，并包装成batch_size大小的batch数据。\n",
    "  #换句话说，一个batch内共有batch_size个序列，每个序列的长度为window_size。\n",
    "  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n",
    "      reader, batch_size=4, window_size=100)\n",
    "\n",
    "  estimator = ts_estimators.TimeSeriesRegressor(\n",
    "      model=_LSTMModel(num_features=1, num_units=128),\n",
    "      optimizer=tf.train.AdamOptimizer(0.001))\n",
    "\n",
    "  estimator.train(input_fn=train_input_fn, steps=2000)\n",
    "  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n",
    "  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n",
    "  (predictions,) = tuple(estimator.predict(\n",
    "      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n",
    "          evaluation, steps=200)))\n",
    "\n",
    "  observed_times = evaluation[\"times\"][0]\n",
    "  observed = evaluation[\"observed\"][0, :, :]\n",
    "  evaluated_times = evaluation[\"times\"][0]\n",
    "  evaluated = evaluation[\"mean\"][0]\n",
    "  predicted_times = predictions['times']\n",
    "  predicted = predictions[\"mean\"]\n",
    "\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  plt.axvline(999, linestyle=\"dotted\", linewidth=4, color='r')\n",
    "  observed_lines = plt.plot(observed_times, observed, label=\"observation\", color=\"k\")\n",
    "  evaluated_lines = plt.plot(evaluated_times, evaluated, label=\"evaluation\", color=\"g\")\n",
    "  predicted_lines = plt.plot(predicted_times, predicted, label=\"prediction\", color=\"r\")\n",
    "  plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],\n",
    "             loc=\"upper left\")\n",
    "  #保存结果\n",
    "  plt.savefig('predict_result.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
